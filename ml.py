# -*- coding: utf-8 -*-
"""Untitled21.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AAC3bXOc54VRP38YNNpwQw3HI-AceMgg
"""

import pandas as pd
import numpy as np
import re
import nltk
from nltk.corpus import stopwords
# load the dataset
df = pd.read_csv('/content/merged_dataset.csv')
print("Missing Values:")
print(df.isnull().sum())

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load your dataset (replace 'your_dataset.csv' with your actual file path)
# Ensure that the file has 'Text' and 'Language' columns or modify the column names in the code below
data = pd.read_csv('/content/merged_dataset.csv')  # Replace with your dataset path

# Display basic information
print("Dataset Info:")
print(data.info())

# Check for missing values
print("\nMissing Values:")
print(data.isnull().sum())

# Add a column for text length
data['Text_Length'] = data['Text'].str.len()

# Basic statistics for text length
print("\nText Length Statistics:")
print(data['Text_Length'].describe())

# Check the top languages
language_counts = data['Language'].value_counts()
print("\nTop Languages by Frequency:")
print(language_counts.head(20))

# Plot the distribution of text lengths
plt.figure(figsize=(10, 6))
sns.histplot(data['Text_Length'], bins=50, kde=True, color='blue')
plt.title("Distribution of Text Lengths")
plt.xlabel("Text Length")
plt.ylabel("Frequency")
plt.show()

# Plot the top languages
top_languages = language_counts.head(20)
plt.figure(figsize=(12, 8))
sns.barplot(x=top_languages.values, y=top_languages.index, palette='viridis')
plt.title("Top Languages by Frequency")
plt.xlabel("Frequency")
plt.ylabel("Language")
plt.show()

# Drop rows with missing values
df.dropna(inplace=True)
print("\nDuplicate Rows:")
print(df.duplicated().sum())

# Drop duplicate rows
df.drop_duplicates(inplace=True)
df['Text'] = df['Text'].apply(lambda x: re.sub(r'[^a-zA-Z0-9\s]', '', x))

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))
df['Text'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))

df['Text'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if len(word) > 2]))

# Task 4: Convert Text to Lowercase
df['Text'] = df['Text'].apply(lambda x: x.lower())

print("\nDataset after cleaning and preprocessing:")
print(df.head())

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import pandas as pd

# Load the dataset
df = pd.read_csv('/content/merged_dataset.csv')

# Define the documents and labels
documents = df['Text']
labels = df['Language']

# Initialize the TfidfVectorizer with N-gram
vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2))

# Fit and transform the documents
ngram = vectorizer.fit_transform(documents)

# Convert the N-gram features to a DataFrame
ngram_df = pd.DataFrame(ngram.toarray(), columns=vectorizer.get_feature_names_out())

# Initialize the LabelEncoder
le = LabelEncoder()

# Fit and transform the labels
encoded_labels = le.fit_transform(labels)

# Print the N-gram features and encoded labels
print("N-gram Features:")
print(ngram_df.head())
print("\nEncoded Labels:")
print(encoded_labels)

# To perform feature extraction and label encoding on a text dataset.


import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('/content/merged_dataset.csv')

# Clean the text data
df['Cleaned Text'] = df['Text'].apply(lambda x: x.lower().strip())
df['Cleaned Text'] = df['Cleaned Text'].apply(lambda x: ''.join(e for e in x if e.isalnum() or e.isspace()))

# Print the cleaned text data
print(df['Cleaned Text'])

# Perform label encoding on the language data
le = LabelEncoder()
df['Language Encoded'] = le.fit_transform(df['Language'])

# Print the encoded language data
print(df[['Language', 'Language Encoded']])

# Perform feature extraction using Bag of Words
bow_vectorizer = CountVectorizer()
bow_features = bow_vectorizer.fit_transform(df['Cleaned Text'])

# Save the BoW features to a file
import scipy.sparse as sparse
sparse.save_npz('bow_features.npz', bow_features)

# Bag of words
import pandas as pd
from sklearn.feature_extraction.text import CountVectorizer

# Load the dataset
df = pd.read_csv('/content/merged_dataset.csv')

# Perform feature extraction using Bag of Words
cv = CountVectorizer(max_features=32337)
bow_features = cv.fit_transform(df['Text'])

# Convert the BoW features to a DataFrame
bow_df = pd.DataFrame(bow_features.toarray(), columns=cv.get_feature_names_out())

# Print the BoW features
print("Bag of Words Features:")
print(bow_df)

# Tf-Idf
import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer

# Load the dataset
df = pd.read_csv('/content/merged_dataset.csv')

# Perform TF-IDF feature extraction
tfidf_vectorizer = TfidfVectorizer(max_features=32337)
tfidf_features = tfidf_vectorizer.fit_transform(df['Text'])

# Convert TF-IDF features to a DataFrame
tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Print TF-IDF features
print("TF-IDF Features:")
print(tfidf_df.head(32337))

#train  multiple ml algorthms
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier

# Load dataset
file_path = 'merged_dataset.csv'  # Update with your file path
data = pd.read_csv(file_path)

# Label Encoding for 'Language'
label_encoder = LabelEncoder()
data['Language_Encoded'] = label_encoder.fit_transform(data['Language'])

# Feature Extraction (TF-IDF)
vectorizer_tfidf = TfidfVectorizer(max_features=5000)  # Limit features for efficiency
X = vectorizer_tfidf.fit_transform(data['Text']).toarray()
y = data['Language_Encoded']

# Train-Test Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize models
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

# Train and evaluate models
for model_name, model in models.items():
    print(f"\nTraining {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"Performance of {model_name}:")
    print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))

# comparing their performance
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import pandas as pd

# Train-Test Split (assuming X_train, X_test, y_train, y_test are defined)
# Define models to train
models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42)
}

# Dictionary to store results
results = []

# Train and evaluate each model
for model_name, model in models.items():
    print(f"\nTraining {model_name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calculate metrics
    accuracy = accuracy_score(y_test, y_pred)
    report = classification_report(y_test, y_pred, target_names=label_encoder.classes_, output_dict=True)

    # Store results
    results.append({
        "Model": model_name,
        "Accuracy": accuracy,
        "Precision (Macro Avg)": report["macro avg"]["precision"],
        "Recall (Macro Avg)": report["macro avg"]["recall"],
        "F1-Score (Macro Avg)": report["macro avg"]["f1-score"]
    })

# Convert results to a DataFrame for easy comparison
results_df = pd.DataFrame(results)

# Display comparison table
print("\nModel Comparison:")
print(results_df)