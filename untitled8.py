# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DTUDLvsgt0gcq9Z3zXZj8wEk1yOhJyMY
"""

# Import required libraries
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
import numpy as np
import re

# Load the dataset
file_path = 'merged_dataset.csv'  # Update with your file path
data = pd.read_csv(file_path)

# Step 1: Data Cleaning
def clean_text(text):
    # Remove special characters, digits, and extra spaces
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Keep only letters and spaces
    text = text.lower().strip()             # Convert to lowercase and strip spaces
    text = re.sub(r'\s+', ' ', text)        # Replace multiple spaces with single space
    return text

# Drop rows with missing or empty text
data = data.dropna(subset=['Text', 'Language'])
data = data[data['Text'].str.strip() != '']

# Apply text cleaning
data['Cleaned_Text'] = data['Text'].apply(clean_text)

# Display cleaned data preview
print("Cleaned Data Preview:")
print(data[['Cleaned_Text', 'Language']].head())

# Step 2: Tokenization
tokenizer = Tokenizer(oov_token="<OOV>")  # Handle out-of-vocabulary words
tokenizer.fit_on_texts(data['Cleaned_Text'])  # Fit tokenizer on cleaned text

# Convert text to sequences
sequences = tokenizer.texts_to_sequences(data['Cleaned_Text'])

# Step 3: Padding
max_length = 50  # Set max length for padding
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')

print("\nExample of Padded Sequence:")
print(padded_sequences[:5])

# Step 4: Encode Labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(data['Language'])

print("\nEncoded Labels:")
print(labels[:5])

# Final Preprocessed Data
X = np.array(padded_sequences)  # Feature matrix
y = np.array(labels)            # Labels

print(f"\nShape of Features: {X.shape}")
print(f"Shape of Labels: {y.shape}")

# Additional Information
print(f"Vocabulary Size: {len(tokenizer.word_index) + 1}")  # +1 for OOV token
print(f"Number of Classes: {len(label_encoder.classes_)}")

# Save the Cleaned and Preprocessed Data
cleaned_file_path = "cleaned_dataset.csv"
data[['Cleaned_Text', 'Language']].to_csv(cleaned_file_path, index=False)

np.save("X_padded_sequences.npy", X)
np.save("y_encoded_labels.npy", y)

print("\nCleaned and preprocessed data saved successfully!")

# Import required libraries
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.preprocessing import LabelEncoder
import numpy as np

# Load the dataset
file_path = 'merged_dataset.csv'  # Update with your file path
data = pd.read_csv(file_path)

# Inspect the dataset
print("Dataset Preview:")
print(data.head())

# Step 1: Tokenization
tokenizer = Tokenizer(oov_token="<OOV>")  # Handle out-of-vocabulary words
tokenizer.fit_on_texts(data['Text'])      # Fit the tokenizer on the 'Text' column

# Convert text to sequences
sequences = tokenizer.texts_to_sequences(data['Text'])

# Step 2: Padding
# Pad sequences to ensure uniform length (choose a suitable maxlen)
max_length = 50
padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')

print("\nExample of Padded Sequence:")
print(padded_sequences[:5])

# Step 3: Encode Labels
label_encoder = LabelEncoder()
labels = label_encoder.fit_transform(data['Language'])

print("\nEncoded Labels:")
print(labels[:5])

# Final Preprocessed Data
X = np.array(padded_sequences)  # Feature matrix
y = np.array(labels)            # Labels

print(f"\nShape of Features: {X.shape}")
print(f"Shape of Labels: {y.shape}")

# Additional Information
print(f"Vocabulary Size: {len(tokenizer.word_index) + 1}")  # +1 for OOV token
print(f"Number of Classes: {len(label_encoder.classes_)}")

# Optional: Save Preprocessed Data
np.save("X_padded_sequences.npy", X)
np.save("y_encoded_labels.npy", y)
print("\nPreprocessed data saved successfully!")

neural network architecture

# Import required libraries
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Define the model architecture
model = Sequential([
    Embedding(input_dim=10000, output_dim=128, input_length=100),  # Embedding layer
    Dropout(0.2),  # Dropout for regularization
    LSTM(64, return_sequences=False),  # LSTM layer with 64 units
    Dense(32, activation='relu'),  # Dense layer with ReLU activation
    Dropout(0.2),  # Additional dropout
    Dense(len(set(y_train)), activation='softmax')  # Output layer with softmax activation
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Display model summary
model.summary()

# Implement early stopping to prevent overfitting
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

# Train the model
history = model.fit(
    X_train_padded,
    y_train_encoded,
    validation_data=(X_test_padded, y_test_encoded),
    epochs=5,  # Adjust epochs as needed
    batch_size=32,  # Default batch size
    callbacks=[early_stopping]
)

# Evaluate the model on the test set
test_loss, test_accuracy = model.evaluate(X_test_padded, y_test_encoded, verbose=2)
print(f"Test Loss: {test_loss}")
print(f"Test Accuracy: {test_accuracy}")

# Import required libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Step 1: Load and preprocess the dataset
file_path = "cleaned_dataset.csv"  # Replace with your file path
data = pd.read_csv(file_path)

# Ensure the necessary columns are present
if 'Cleaned_Text' not in data.columns or 'Language' not in data.columns:
    raise ValueError("Dataset must contain 'Cleaned_Text' and 'Language' columns.")

# Extract features (X) and target labels (y)
X = data['Cleaned_Text'].astype(str).values  # Ensure text is string
y = data['Language'].values  # Labels

# Step 2: Encode labels into integers
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)
num_classes = len(label_encoder.classes_)  # Number of unique classes

# Step 3: Split the data into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Step 4: Tokenization and padding
vocab_size = 10000  # Maximum number of words in the vocabulary
oov_token = "<OOV>"  # Token for out-of-vocabulary words
max_length = 100  # Max sequence length
padding_type = 'post'
trunc_type = 'post'

# Initialize and fit the tokenizer
tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_token)
tokenizer.fit_on_texts(X_train)

# Convert text to sequences
X_train_seq = tokenizer.texts_to_sequences(X_train)
X_test_seq = tokenizer.texts_to_sequences(X_test)

# Pad sequences to ensure uniform input size
X_train_padded = pad_sequences(X_train_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)
X_test_padded = pad_sequences(X_test_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# Step 5: Build the LSTM model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_length),  # Word embedding layer
    Dropout(0.2),  # Dropout for regularization
    LSTM(64, return_sequences=False),  # LSTM layer with 64 units
    Dense(32, activation='relu'),  # Hidden dense layer with ReLU activation
    Dropout(0.2),  # Additional dropout layer
    Dense(num_classes, activation='softmax')  # Output layer for multi-class classification
])

# Compile the model
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# Display model summary
model.summary()

# Step 6: Train the model
early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)

history = model.fit(
    X_train_padded,
    y_train,
    validation_data=(X_test_padded, y_test),
    epochs=10,  # You can increase epochs for better results
    batch_size=32,  # Standard batch size
    callbacks=[early_stopping]
)

# Step 7: Evaluate the model on test data
test_loss, test_accuracy = model.evaluate(X_test_padded, y_test, verbose=2)
print(f"Test Loss: {test_loss:.4f}")
print(f"Test Accuracy: {test_accuracy:.4f}")

# Step 8: Save the model and tokenizer
model.save("lstm_text_classification_model.h5")
import pickle
with open("tokenizer.pkl", "wb") as tokenizer_file:
    pickle.dump(tokenizer, tokenizer_file)

print("Model and tokenizer saved successfully!")

# Optional: Save label encoder for decoding predictions
with open("label_encoder.pkl", "wb") as label_file:
    pickle.dump(label_encoder, label_file)

# Step 9: Example prediction
sample_text = ["This is an example sentence for prediction."]
sample_seq = tokenizer.texts_to_sequences(sample_text)
sample_padded = pad_sequences(sample_seq, maxlen=max_length, padding=padding_type, truncating=trunc_type)

# Predict class
prediction = model.predict(sample_padded)
predicted_class = label_encoder.inverse_transform([np.argmax(prediction)])
print(f"Predicted Class: {predicted_class[0]}")

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Load the dataset
df = pd.read_csv('/content/merged_dataset.csv')

# Ensure the dataset has the required columns
print(df.head())  # Check the first few rows
print(df.info())  # Check the column names and data types

# Replace 'Text' and 'Language' with your actual column names
X = df['Text']  # Features: Text data
y = df['Language']  # Labels: Language categories

# Encode the labels into integers
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# Split the data into training and testing sets
x_train, x_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Tokenize the text data
tokenizer = Tokenizer(num_words=10000, oov_token="<OOV>")
tokenizer.fit_on_texts(x_train)

# Convert text to sequences
x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq = tokenizer.texts_to_sequences(x_test)

# Define the maximum length for padding
max_length = 100

# Apply padding to sequences
x_train_padded = pad_sequences(x_train_seq, maxlen=max_length, padding='post', truncating='post')
x_test_padded = pad_sequences(x_test_seq, maxlen=max_length, padding='post', truncating='post')

# Print the vocabulary size
vocab_size = len(tokenizer.word_index) + 1
print(f"Vocabulary Size: {vocab_size}")

# Print examples of tokenized and padded sequences
print("Example of Tokenized Sequence (Train):", x_train_seq[:1])
print("Example of Padded Sequence (Train):", x_train_padded[:1])
print("Example of Tokenized Sequence (Test):", x_test_seq[:1])
print("Example of Padded Sequence (Test):", x_test_padded[:1])

# Build the neural network model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=64, input_length=max_length),
    LSTM(64, return_sequences=False),
    Dense(32, activation='relu'),
    Dense(len(label_encoder.classes_), activation='softmax')  # Adjust based on the number of unique labels
])

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(x_train_padded, y_train, epochs=10, validation_data=(x_test_padded, y_test))

# Evaluate the model
loss, accuracy = model.evaluate(x_test_padded, y_test)
print(f"Test Accuracy: {accuracy:.2f}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout

# Load the dataset
dataset = pd.read_csv('/content/merged_dataset.csv')  # Update with the correct path to the dataset

# Step 1: Preprocessing
def preprocess_data(dataset, max_words=10000, max_sequence_length=100):
    # Tokenize the text data
    tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
    tokenizer.fit_on_texts(dataset['Text'])
    sequences = tokenizer.texts_to_sequences(dataset['Text'])
    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

    # Encode the labels
    label_encoder = LabelEncoder()
    labels = label_encoder.fit_transform(dataset['Language'])

    return padded_sequences, labels, tokenizer, label_encoder

# Preprocess the dataset
max_words = 10000
max_sequence_length = 100
padded_sequences, labels, tokenizer, label_encoder = preprocess_data(dataset, max_words, max_sequence_length)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Step 2: Define Models
def create_lstm_model(input_length, vocab_size, output_classes):
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length),
        LSTM(128, return_sequences=False),
        Dropout(0.2),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(output_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def create_gru_model(input_length, vocab_size, output_classes):
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length),
        GRU(128, return_sequences=False),
        Dropout(0.2),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(output_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Prepare input dimensions
input_length = max_sequence_length
vocab_size = max_words
output_classes = len(label_encoder.classes_)

# Create LSTM and GRU models
lstm_model = create_lstm_model(input_length, vocab_size, output_classes)
gru_model = create_gru_model(input_length, vocab_size, output_classes)

# Step 3: Training
def train_model(model, X_train, y_train, X_test, y_test, epochs=20, batch_size=32):
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=batch_size,
        verbose=1
    )
    return history

# Train LSTM model
print("Training LSTM model...")
lstm_history = train_model(lstm_model, X_train, y_train, X_test, y_test)

# Train GRU model
print("Training GRU model...")
gru_history = train_model(gru_model, X_train, y_train, X_test, y_test)

# Step 4: Evaluation
def evaluate_model(model, X_test, y_test):
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    return loss, accuracy

lstm_loss, lstm_accuracy = evaluate_model(lstm_model, X_test, y_test)
gru_loss, gru_accuracy = evaluate_model(gru_model, X_test, y_test)

print(f"LSTM Model - Loss: {lstm_loss}, Accuracy: {lstm_accuracy}")
print(f"GRU Model - Loss: {gru_loss}, Accuracy: {gru_accuracy}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout

# Load the dataset
dataset = pd.read_csv('/content/merged_dataset.csv')  # Update with the correct path to the dataset

# Step 1: Preprocessing
def preprocess_data(dataset, max_words=10000, max_sequence_length=100):
    # Tokenize the text data
    tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
    tokenizer.fit_on_texts(dataset['Text'])
    sequences = tokenizer.texts_to_sequences(dataset['Text'])
    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

    # Encode the labels
    label_encoder = LabelEncoder()
    labels = label_encoder.fit_transform(dataset['Language'])

    return padded_sequences, labels, tokenizer, label_encoder

# Preprocess the dataset
max_words = 10000
max_sequence_length = 100
padded_sequences, labels, tokenizer, label_encoder = preprocess_data(dataset, max_words, max_sequence_length)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Step 2: Define Models
def create_lstm_model(input_length, vocab_size, output_classes):
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length),
        LSTM(128, return_sequences=False),
        Dropout(0.2),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(output_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

def create_gru_model(input_length, vocab_size, output_classes):
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length),
        GRU(128, return_sequences=False),
        Dropout(0.2),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(output_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Prepare input dimensions
input_length = max_sequence_length
vocab_size = max_words
output_classes = len(label_encoder.classes_)

# Create LSTM and GRU models
lstm_model = create_lstm_model(input_length, vocab_size, output_classes)
gru_model = create_gru_model(input_length, vocab_size, output_classes)

# Step 3: Training
def train_model(model, X_train, y_train, X_test, y_test, epochs=20, batch_size=32):
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=batch_size,
        verbose=1
    )
    return history

# Train LSTM model
print("Training LSTM model...")
lstm_history = train_model(lstm_model, X_train, y_train, X_test, y_test)

# Evaluate LSTM model
def evaluate_model(model, X_test, y_test):
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    return loss, accuracy

lstm_loss, lstm_accuracy = evaluate_model(lstm_model, X_test, y_test)
print(f"LSTM Model - Loss: {lstm_loss}, Accuracy: {lstm_accuracy}")

# Step 4: Prediction and Detailed Accuracy Analysis
def analyze_language_accuracy(model, X_test, y_test, label_encoder):
    predictions = model.predict(X_test, verbose=0)
    predicted_labels = np.argmax(predictions, axis=1)
    true_labels = y_test

    class_accuracies = {}
    for label_index, label_name in enumerate(label_encoder.classes_):
        label_mask = (true_labels == label_index)
        class_accuracy = np.mean(predicted_labels[label_mask] == true_labels[label_mask])
        class_accuracies[label_name] = class_accuracy

    return class_accuracies

# Analyze per-language accuracy
language_accuracies = analyze_language_accuracy(lstm_model, X_test, y_test, label_encoder)

# Print accuracies and identify low-performing languages
print("Language-wise accuracies:")
for language, accuracy in language_accuracies.items():
    print(f"{language}: {accuracy:.2f}")

# Step 5: Improve Low Accuracy Languages
def augment_low_accuracy_languages(dataset, tokenizer, label_encoder, low_accuracy_languages, max_sequence_length):
    augmented_data = []
    for _, row in dataset.iterrows():
        if row['Language'] in low_accuracy_languages:
            # Example augmentation: add noise or synonyms to text
            text = row['Text'] + " " + row['Text']  # Duplicate text as a simple augmentation
            label = row['Language']
            augmented_data.append((text, label))

    if augmented_data:
        augmented_df = pd.DataFrame(augmented_data, columns=['Text', 'Language'])
        dataset = pd.concat([dataset, augmented_df], ignore_index=True)

    # Reprocess dataset
    padded_sequences, labels, _, _ = preprocess_data(dataset, max_words=max_words, max_sequence_length=max_sequence_length)
    return train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

low_accuracy_languages = [lang for lang, acc in language_accuracies.items() if acc < 0.8]
if low_accuracy_languages:
    print("Improving accuracy for low-performing languages:", low_accuracy_languages)
    X_train, X_test, y_train, y_test = augment_low_accuracy_languages(dataset, tokenizer, label_encoder, low_accuracy_languages, max_sequence_length)

    # Retrain the model
    print("Retraining LSTM model...")
    lstm_model = create_lstm_model(input_length, vocab_size, output_classes)
    lstm_history = train_model(lstm_model, X_train, y_train, X_test, y_test)

    # Reevaluate the model
    lstm_loss, lstm_accuracy = evaluate_model(lstm_model, X_test, y_test)
    print(f"Retrained LSTM Model - Loss: {lstm_loss}, Accuracy: {lstm_accuracy}")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Embedding, GRU, Dense, Dropout

# Load the dataset
dataset = pd.read_csv('/content/merged_dataset.csv')  # Update with the correct path to the dataset

# Step 1: Preprocessing
def preprocess_data(dataset, max_words=10000, max_sequence_length=100):
    # Tokenize the text data
    tokenizer = Tokenizer(num_words=max_words, oov_token="<OOV>")
    tokenizer.fit_on_texts(dataset['Text'])
    sequences = tokenizer.texts_to_sequences(dataset['Text'])
    padded_sequences = pad_sequences(sequences, maxlen=max_sequence_length, padding='post')

    # Encode the labels
    label_encoder = LabelEncoder()
    labels = label_encoder.fit_transform(dataset['Language'])

    return padded_sequences, labels, tokenizer, label_encoder

# Preprocess the dataset
max_words = 10000
max_sequence_length = 100
padded_sequences, labels, tokenizer, label_encoder = preprocess_data(dataset, max_words, max_sequence_length)

# Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

# Step 2: Define GRU Model
def create_gru_model(input_length, vocab_size, output_classes):
    model = Sequential([
        Embedding(input_dim=vocab_size, output_dim=128, input_length=input_length),
        GRU(128, return_sequences=False),
        Dropout(0.2),
        Dense(128, activation='relu'),
        Dropout(0.2),
        Dense(output_classes, activation='softmax')
    ])
    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
    return model

# Prepare input dimensions
input_length = max_sequence_length
vocab_size = max_words
output_classes = len(label_encoder.classes_)

# Create GRU model
gru_model = create_gru_model(input_length, vocab_size, output_classes)

# Step 3: Training
def train_model(model, X_train, y_train, X_test, y_test, epochs=20, batch_size=32):
    history = model.fit(
        X_train, y_train,
        validation_data=(X_test, y_test),
        epochs=epochs,
        batch_size=batch_size,
        verbose=1
    )
    return history

# Train GRU model
print("Training GRU model...")
gru_history = train_model(gru_model, X_train, y_train, X_test, y_test)

# Save the GRU model
gru_model.save('gru_model.h5')
print("GRU model saved as gru_model.h5")

# Evaluate GRU model
def evaluate_model(model, X_test, y_test):
    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)
    return loss, accuracy

gru_loss, gru_accuracy = evaluate_model(gru_model, X_test, y_test)
print(f"GRU Model - Loss: {gru_loss}, Accuracy: {gru_accuracy}")

# Step 4: Prediction and Detailed Accuracy Analysis
def analyze_language_accuracy(model, X_test, y_test, label_encoder):
    predictions = model.predict(X_test, verbose=0)
    predicted_labels = np.argmax(predictions, axis=1)
    true_labels = y_test

    class_accuracies = {}
    for label_index, label_name in enumerate(label_encoder.classes_):
        label_mask = (true_labels == label_index)
        class_accuracy = np.mean(predicted_labels[label_mask] == true_labels[label_mask])
        class_accuracies[label_name] = class_accuracy

    return class_accuracies

# Analyze per-language accuracy
language_accuracies = analyze_language_accuracy(gru_model, X_test, y_test, label_encoder)

# Print accuracies and identify low-performing languages
print("Language-wise accuracies:")
for language, accuracy in language_accuracies.items():
    print(f"{language}: {accuracy:.2f}")

# Step 5: Improve Low Accuracy Languages
def augment_low_accuracy_languages(dataset, tokenizer, label_encoder, low_accuracy_languages, max_sequence_length):
    augmented_data = []
    for _, row in dataset.iterrows():
        if row['Language'] in low_accuracy_languages:
            # Example augmentation: add noise or synonyms to text
            text = row['Text'] + " " + row['Text']  # Duplicate text as a simple augmentation
            label = row['Language']
            augmented_data.append((text, label))

    if augmented_data:
        augmented_df = pd.DataFrame(augmented_data, columns=['Text', 'Language'])
        dataset = pd.concat([dataset, augmented_df], ignore_index=True)

    # Reprocess dataset
    padded_sequences, labels, _, _ = preprocess_data(dataset, max_words=max_words, max_sequence_length=max_sequence_length)
    return train_test_split(padded_sequences, labels, test_size=0.2, random_state=42)

low_accuracy_languages = [lang for lang, acc in language_accuracies.items() if acc < 0.8]
if low_accuracy_languages:
    print("Improving accuracy for low-performing languages:", low_accuracy_languages)
    X_train, X_test, y_train, y_test = augment_low_accuracy_languages(dataset, tokenizer, label_encoder, low_accuracy_languages, max_sequence_length)

    # Retrain the model
    print("Retraining GRU model...")
    gru_model = create_gru_model(input_length, vocab_size, output_classes)
    gru_history = train_model(gru_model, X_train, y_train, X_test, y_test)

    # Save the retrained GRU model
    gru_model.save('retrained_gru_model.h5')
    print("Retrained GRU model saved as retrained_gru_model.h5")

    # Reevaluate the model
    gru_loss, gru_accuracy = evaluate_model(gru_model, X_test, y_test)
    print(f"Retrained GRU Model - Loss: {gru_loss}, Accuracy: {gru_accuracy}")