{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "038cee11-58a9-4035-b55a-b1df8ab6dda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langdetect import detect\\nfrom tensorflow.keras.models import load_model\\nimport numpy as np\\nfrom tensorflow.keras.preprocessing.text import tokenizer_from_json\\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\\nimport pickle\\nimport json\\n\\n# Load the trained model\\nmodel = load_model(\"C:/Users/Muhammad Mitkar/Desktop/LSN/gru_language_detection_model.h5\")\\n\\n# Define the max sequence length (this should match the length used during training)\\nMAX_SEQUENCE_LENGTH = 100\\n\\n# Load the tokenizer from the saved JSON file\\nwith open(\"C:/Users/Muhammad Mitkar/Desktop/LSN/tokenizer.json\", \"r\") as f:\\n    tokenizer_json = json.load(f)\\n    tokenizer = tokenizer_from_json(tokenizer_json)\\n\\n# Load the label encoder\\nwith open(\"C:/Users/Muhammad Mitkar/Desktop/LSN/label_encoder.pkl\", \"rb\") as f:\\n    label_encoder = pickle.load(f)\\n\\n# Preprocessing function: Tokenize and pad the input text\\ndef preprocess_input(input_text):\\n    # Tokenize and pad the input text\\n    tokenized_input = tokenizer.texts_to_sequences([input_text])\\n    padded_input = pad_sequences(tokenized_input, maxlen=MAX_SEQUENCE_LENGTH, padding=\\'post\\')\\n    return padded_input\\n\\n# Prediction function: Predict the language\\ndef predict_language(input_text):\\n    # Preprocess the input text\\n    preprocessed_input = preprocess_input(input_text)\\n    \\n    # Predict using the model\\n    predictions = model.predict(preprocessed_input)\\n    \\n    # Convert predictions to language label using the label encoder\\n    predicted_language = label_encoder.inverse_transform([np.argmax(predictions)])\\n    \\n    return predicted_language[0]\\n\\n# Function to detect language of each sentence and predict languages\\ndef predict_multilingual(input_text):\\n    # Split the text into sentences or parts\\n    sentences = input_text.split(\\'.\\')  # Split by period (or use other logic to split based on punctuation or space)\\n\\n    # Set to store detected languages to avoid duplicates\\n    detected_languages = set()\\n\\n    for sentence in sentences:\\n        if sentence.strip():  # Avoid empty sentences\\n            lang = detect(sentence)  # Detect the language of the sentence\\n            \\n            # Predict the language using your model (optional for more accuracy)\\n            predicted_lang = predict_language(sentence)\\n            \\n            # Add the predicted language to the set (avoids duplicates)\\n            detected_languages.add(predicted_lang)\\n\\n    return detected_languages\\n\\n# Example input with mixed languages\\ninput_text = \"தனது வாழ்க்கையில் பல சவால்களையும் தடைகளையும்   சந்தித்தாலும், ஒருபோதும் தனது கனவுகளை கைவிடாமல்.This is a simple test which needs a cute ghapap.Несмотря на многочисленные трудности и неудачи, с которыми он сталкивался на протяжении своей карьеры, ؟\"\\n\\n# Get the predicted languages\\ndetected_languages = predict_multilingual(input_text)\\n\\n# Output only the detected languages\\nfor language in detected_languages:\\n    print(f\"Detected Language: {language}\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from langdetect import detect\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"C:/Users/Muhammad Mitkar/Desktop/LSN/gru_language_detection_model.h5\")\n",
    "\n",
    "# Define the max sequence length (this should match the length used during training)\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# Load the tokenizer from the saved JSON file\n",
    "with open(\"C:/Users/Muhammad Mitkar/Desktop/LSN/tokenizer.json\", \"r\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "# Load the label encoder\n",
    "with open(\"C:/Users/Muhammad Mitkar/Desktop/LSN/label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Preprocessing function: Tokenize and pad the input text\n",
    "def preprocess_input(input_text):\n",
    "    # Tokenize and pad the input text\n",
    "    tokenized_input = tokenizer.texts_to_sequences([input_text])\n",
    "    padded_input = pad_sequences(tokenized_input, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    return padded_input\n",
    "\n",
    "# Prediction function: Predict the language\n",
    "def predict_language(input_text):\n",
    "    # Preprocess the input text\n",
    "    preprocessed_input = preprocess_input(input_text)\n",
    "    \n",
    "    # Predict using the model\n",
    "    predictions = model.predict(preprocessed_input)\n",
    "    \n",
    "    # Convert predictions to language label using the label encoder\n",
    "    predicted_language = label_encoder.inverse_transform([np.argmax(predictions)])\n",
    "    \n",
    "    return predicted_language[0]\n",
    "\n",
    "# Function to detect language of each sentence and predict languages\n",
    "def predict_multilingual(input_text):\n",
    "    # Split the text into sentences or parts\n",
    "    sentences = input_text.split('.')  # Split by period (or use other logic to split based on punctuation or space)\n",
    "\n",
    "    # Set to store detected languages to avoid duplicates\n",
    "    detected_languages = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():  # Avoid empty sentences\n",
    "            lang = detect(sentence)  # Detect the language of the sentence\n",
    "            \n",
    "            # Predict the language using your model (optional for more accuracy)\n",
    "            predicted_lang = predict_language(sentence)\n",
    "            \n",
    "            # Add the predicted language to the set (avoids duplicates)\n",
    "            detected_languages.add(predicted_lang)\n",
    "\n",
    "    return detected_languages\n",
    "\n",
    "# Example input with mixed languages\n",
    "input_text = \"தனது வாழ்க்கையில் பல சவால்களையும் தடைகளையும்   சந்தித்தாலும், ஒருபோதும் தனது கனவுகளை கைவிடாமல்.This is a simple test which needs a cute ghapap.Несмотря на многочисленные трудности и неудачи, с которыми он сталкивался на протяжении своей карьеры, ؟\"\n",
    "\n",
    "# Get the predicted languages\n",
    "detected_languages = predict_multilingual(input_text)\n",
    "\n",
    "# Output only the detected languages\n",
    "for language in detected_languages:\n",
    "    print(f\"Detected Language: {language}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6789ea23-b73b-4d95-80bd-09362ddcacf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 544ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "Detected Language: English\n",
      "  Sentence: She loves to travel around the world and this is why it so cute but the only problem is it, and she hopes to visit Paris one day\n",
      "Detected Language: French\n",
      "  Sentence: Elle rêve de découvrir de nouveaux endroits, y también le gustaría aprender español\n",
      "  Sentence: ，et ensuite nous avons pris un café\n",
      "Detected Language: Pushto\n",
      "  Sentence: که څه هم هغه په خپل ژوند کې له ډیرو ستونزو او سختیو سره مخ شو،\n",
      "Detected Language: Indonesian\n",
      "  Sentence: Meskipun dia menghadapi banyak tantangan dan kegagalan dalam hidupnya, dia tidak pernah menyerah dan terus bekerja keras dengan tekad yang kuat\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model(\"C:/Users/Muhammad Mitkar/Desktop/LSN/gru_language_detection_model.h5\")\n",
    "\n",
    "# Define the max sequence length (this should match the length used during training)\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# Load the tokenizer from the saved JSON file\n",
    "with open(\"C:/Users/Muhammad Mitkar/Desktop/LSN/tokenizer.json\", \"r\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "# Load the label encoder\n",
    "with open(\"C:/Users/Muhammad Mitkar/Desktop/LSN/label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)\n",
    "\n",
    "# Preprocessing function: Tokenize and pad the input text\n",
    "def preprocess_input(input_text):\n",
    "    # Tokenize and pad the input text\n",
    "    tokenized_input = tokenizer.texts_to_sequences([input_text])\n",
    "    padded_input = pad_sequences(tokenized_input, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    return padded_input\n",
    "\n",
    "# Prediction function: Predict the language\n",
    "def predict_language(input_text):\n",
    "    # Preprocess the input text\n",
    "    preprocessed_input = preprocess_input(input_text)\n",
    "    \n",
    "    # Predict using the model\n",
    "    predictions = model.predict(preprocessed_input)\n",
    "    \n",
    "    # Convert predictions to language label using the label encoder\n",
    "    predicted_language = label_encoder.inverse_transform([np.argmax(predictions)])\n",
    "    \n",
    "    return predicted_language[0]\n",
    "\n",
    "# Function to detect language of each sentence and predict languages\n",
    "def predict_multilingual(input_text):\n",
    "    # Split the text into sentences or parts\n",
    "    sentences = input_text.split('.')  # Split by period (or use other logic to split based on punctuation or space)\n",
    "\n",
    "    # Dictionary to store detected languages and corresponding sentences\n",
    "    detected_languages = {}\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():  # Avoid empty sentences\n",
    "            lang = detect(sentence)  # Detect the language of the sentence\n",
    "            \n",
    "            # Predict the language using your model (optional for more accuracy)\n",
    "            predicted_lang = predict_language(sentence)\n",
    "            \n",
    "            # If the language is already detected, append the sentence\n",
    "            if predicted_lang not in detected_languages:\n",
    "                detected_languages[predicted_lang] = []\n",
    "            detected_languages[predicted_lang].append(sentence.strip())\n",
    "\n",
    "    return detected_languages\n",
    "\n",
    "# Example input with mixed languages\n",
    "input_text = \"She loves to travel around the world and this is why it so cute but the only problem is it, and she hopes to visit Paris one day. Elle rêve de découvrir de nouveaux endroits, y también le gustaría aprender español.，et ensuite nous avons pris un café.که څه هم هغه په خپل ژوند کې له ډیرو ستونزو او سختیو سره مخ شو،.Meskipun dia menghadapi banyak tantangan dan kegagalan dalam hidupnya, dia tidak pernah menyerah dan terus bekerja keras dengan tekad yang kuat\"\n",
    "\n",
    "# Get the predicted languages and corresponding sentences\n",
    "detected_languages = predict_multilingual(input_text)\n",
    "\n",
    "# Output the result (Languages with corresponding text)\n",
    "for language, sentences in detected_languages.items():\n",
    "    print(f\"Detected Language: {language}\")\n",
    "    for sentence in sentences:\n",
    "        print(f\"  Sentence: {sentence}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a9202f0-163e-4c8b-a2ad-81b8771986d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import tokenizer_from_json\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04cee6e9-9b35-4ab8-b19c-4f1f42f591a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = load_model(\"C:/Users/Muhammad Mitkar/Desktop/LSN/gru_language_detection_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b8e8b30-d473-459a-896c-b3a88f3eed49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the max sequence length (this should match the length used during training)\n",
    "MAX_SEQUENCE_LENGTH = 100\n",
    "\n",
    "# Load the tokenizer from the saved JSON file\n",
    "with open(\"C:/Users/Muhammad Mitkar/Desktop/LSN/tokenizer.json\", \"r\") as f:\n",
    "    tokenizer_json = json.load(f)\n",
    "    tokenizer = tokenizer_from_json(tokenizer_json)\n",
    "\n",
    "# Load the label encoder\n",
    "with open(\"C:/Users/Muhammad Mitkar/Desktop/LSN/label_encoder.pkl\", \"rb\") as f:\n",
    "    label_encoder = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c641e07-c632-441a-bf22-c2f771ac30c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function: Tokenize and pad the input text\n",
    "def preprocess_input(input_text):\n",
    "    # Tokenize and pad the input text\n",
    "    tokenized_input = tokenizer.texts_to_sequences([input_text])\n",
    "    padded_input = pad_sequences(tokenized_input, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
    "    return padded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cc6ee69-8b5f-4b1b-bc7c-c342997986ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction function: Predict the language\n",
    "def predict_language(input_text):\n",
    "    # Preprocess the input text\n",
    "    preprocessed_input = preprocess_input(input_text)\n",
    "    \n",
    "    # Predict using the model\n",
    "    predictions = model.predict(preprocessed_input)\n",
    "    \n",
    "    # Convert predictions to language label using the label encoder\n",
    "    predicted_language = label_encoder.inverse_transform([np.argmax(predictions)])\n",
    "    \n",
    "    return predicted_language[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa5b435-8b4c-40d7-8396-02ed6aa39a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to detect language of each sentence and predict languages\n",
    "def predict_multilingual(input_text):\n",
    "    # Split the text into sentences or parts\n",
    "    sentences = input_text.split('.')  # Split by period (or use other logic to split based on punctuation or space)\n",
    "\n",
    "    # Set to store detected languages to avoid duplicates\n",
    "    detected_languages = set()\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if sentence.strip():  # Avoid empty sentences\n",
    "            lang = detect(sentence)  # Detect the language of the sentence\n",
    "            \n",
    "            # Predict the language using your model (optional for more accuracy)\n",
    "            predicted_lang = predict_language(sentence)\n",
    "            \n",
    "            # Add the predicted language to the set (avoids duplicates)\n",
    "            detected_languages.add(predicted_lang)\n",
    "\n",
    "    return detected_languages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d1ea30d-0a80-48b3-a75f-0ed790c1e5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 799ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Detected Language: English\n",
      "Detected Language: French\n",
      "Detected Language: Indonesian\n",
      "Detected Language: Pushto\n"
     ]
    }
   ],
   "source": [
    "# Example input with mixed languages\n",
    "input_text = \"She loves to travel around the world and this is why it so cute but the only problem is it, and she hopes to visit Paris one day. Elle rêve de découvrir de nouveaux endroits, y también le gustaría aprender español.，et ensuite nous avons pris un café.که څه هم هغه په خپل ژوند کې له ډیرو ستونزو او سختیو سره مخ شو،.Meskipun dia menghadapi banyak tantangan dan kegagalan dalam hidupnya, dia tidak pernah menyerah dan terus bekerja keras dengan tekad yang kuat,\"\n",
    "\n",
    "# Get the predicted languages\n",
    "detected_languages = predict_multilingual(input_text)\n",
    "\n",
    "# Output only the detected languages\n",
    "for language in detected_languages:\n",
    "    print(f\"Detected Language: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8d901-7255-456d-8ff1-26160dedae71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3917d744-ba25-4872-b67b-04bf7f3b8480",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSNENV",
   "language": "python",
   "name": "lsnenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
