{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10279158,"sourceType":"datasetVersion","datasetId":6360636}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-23T13:22:26.769565Z\",\"iopub.execute_input\":\"2024-12-23T13:22:26.769859Z\",\"iopub.status.idle\":\"2024-12-23T13:22:27.082320Z\",\"shell.execute_reply.started\":\"2024-12-23T13:22:26.769835Z\",\"shell.execute_reply\":\"2024-12-23T13:22:27.081586Z\"}}\nimport numpy as np\nimport pandas as pd\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-23T13:22:27.083165Z\",\"iopub.execute_input\":\"2024-12-23T13:22:27.083762Z\",\"iopub.status.idle\":\"2024-12-23T13:22:27.548953Z\",\"shell.execute_reply.started\":\"2024-12-23T13:22:27.083730Z\",\"shell.execute_reply\":\"2024-12-23T13:22:27.547986Z\"}}\ndf = pd.read_csv('/kaggle/input/lingualsense/merged_dataset.csv')\nprint(df.head())\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-23T13:22:27.549752Z\",\"iopub.execute_input\":\"2024-12-23T13:22:27.550056Z\",\"iopub.status.idle\":\"2024-12-23T13:22:27.985866Z\",\"shell.execute_reply.started\":\"2024-12-23T13:22:27.550032Z\",\"shell.execute_reply\":\"2024-12-23T13:22:27.984836Z\"}}\ndf.rename(columns={'Text': 'text', 'Language': 'language'}, inplace=True)\n\ndf.dropna(subset=['text', 'language'], inplace=True)\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ndf['language_encoded'] = label_encoder.fit_transform(df['language'])\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-23T13:22:27.986691Z\",\"iopub.execute_input\":\"2024-12-23T13:22:27.987048Z\",\"iopub.status.idle\":\"2024-12-23T13:22:28.083259Z\",\"shell.execute_reply.started\":\"2024-12-23T13:22:27.987024Z\",\"shell.execute_reply\":\"2024-12-23T13:22:28.082373Z\"}}\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    df['text'], df['language_encoded'], test_size=0.2, random_state=42\n)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-23T13:22:28.084051Z\",\"iopub.execute_input\":\"2024-12-23T13:22:28.084317Z\",\"iopub.status.idle\":\"2024-12-23T13:22:30.755923Z\",\"shell.execute_reply.started\":\"2024-12-23T13:22:28.084294Z\",\"shell.execute_reply\":\"2024-12-23T13:22:30.755128Z\"}}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer(max_features=5000)\nX_train_tfidf = tfidf.fit_transform(X_train).toarray()\nX_test_tfidf = tfidf.transform(X_test).toarray()\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-23T13:22:30.757808Z\",\"iopub.execute_input\":\"2024-12-23T13:22:30.758080Z\",\"iopub.status.idle\":\"2024-12-23T13:22:38.650428Z\",\"shell.execute_reply.started\":\"2024-12-23T13:22:30.758059Z\",\"shell.execute_reply\":\"2024-12-23T13:22:38.649729Z\"}}\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, GRU, Embedding, Dropout\n\n# Define the model\nmodel = Sequential([\n    Dense(128, activation='relu', input_shape=(X_train_tfidf.shape[1],)),\n    Dropout(0.2),\n    Dense(64, activation='relu'),\n    Dropout(0.2),\n    Dense(len(label_encoder.classes_), activation='softmax')\n])\n\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nmodel.summary()\n\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-23T13:22:38.651245Z\",\"iopub.execute_input\":\"2024-12-23T13:22:38.651740Z\",\"iopub.status.idle\":\"2024-12-23T13:23:01.103273Z\",\"shell.execute_reply.started\":\"2024-12-23T13:22:38.651717Z\",\"shell.execute_reply\":\"2024-12-23T13:23:01.102500Z\"}}\nmodel.fit(X_train_tfidf, y_train, validation_data=(X_test_tfidf, y_test), epochs=10, batch_size=32)\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-12-23T13:23:10.331499Z\",\"iopub.execute_input\":\"2024-12-23T13:23:10.331792Z\",\"iopub.status.idle\":\"2024-12-23T13:23:10.471326Z\",\"shell.execute_reply.started\":\"2024-12-23T13:23:10.331769Z\",\"shell.execute_reply\":\"2024-12-23T13:23:10.470646Z\"}}\nimport pickle\n\n# Save the model\nmodel.save('language_detection_gru.h5')\n\n# Save the TF-IDF vectorizer\nwith open('tfidf_vectorizer.pkl', 'wb') as f:\n    pickle.dump(tfidf, f)\n\n# Save the LabelEncoder\nwith open('label_encoder.pkl', 'wb') as f:\n    pickle.dump(label_encoder, f)","metadata":{"_uuid":"c97ad005-f71e-4aaf-8aa3-6e0ccb945a56","_cell_guid":"3247ea86-3590-4656-9141-2c6430fb0617","trusted":true,"collapsed":false,"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":null}]}